---
title: Project-Riddle
layout: page
permalink: /project/riddle/index.html
---


## Usable Interpretability

<hr>

### Overview


<div align="center"><img src="/assets/images/riddle.pdf" alt="padlock" height="50%" width="50%" align="middle"/></div>


The state-of-the-art performance of deep neural networks (DNNs) is often achieved at the cost of their interpretability. This is a major drawback for domains where the interpretability of decisions is a critical prerequisite. While a plethora of interpretation models have been proposed to help users understand the inner workings of DNNs, their interpretability is far from being practically useful. The goal of this project is to fundamentally improve the usability of interpretable deep learning system (IDLS) along (i) reliability - the interpretation should be robust against adversarial manipulations, (ii) interactivity - the interpretation should account for the perception, understanding, and response of end users, and (iii) operability - the interpretation should serve as the lens for end users to effectively understand and control DNN behaviors. 


<hr>

### Publications

*  **<font color="purple"> AdvMind: Inferring Adversary Intent of Black-Box Attacks </font>** [[pdf](https://arxiv.org/pdf/2006.09539.pdf)]  <br>
*<font color="black">Ren Pang, Xinyang Zhang, Shouling Ji, Xiapu Luo, Ting Wang</font>* <br>
*<font color="blue">The 26th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'20)</font>*


*  **<font color="purple"> A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models </font>** [[pdf](https://arxiv.org/pdf/1911.01559.pdf)]  <br>
*<font color="black">Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Alex Liu, Ting Wang</font>* <br>
*<font color="blue">The 27th ACM Conference on Computer and Communications Security (CCS'20)</font>*
s
*  **<font color="purple">  Text Captcha Is Dead? A Large Scale Deployment and Empirical Study </font>** [[pdf]({{ site.url }}/paper/Shi-ccs-2020.pdf)]  <br>
*<font color="black">Chenghui Shi, Shouling Ji, Qianjun Liu, Changchang Liu, Yuefeng Chen, Yuan He, Zhe Liu, Raheem Beyah,
Ting Wang</font>* <br>
*<font color="blue">The 27th ACM Conference on Computer and Communications Security (CCS'20)</font>*

*  **<font color="purple"> TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation </font>** [[pdf]({{ site.url }}/paper/Li-usenix-2020.pdf)]  <br>
*<font color="black">Jinfeng Li, Tianyu Du, Shouling Ji, Rong Zhang, Quan Lu, Min Yang, Ting Wang</font>* <br>
*<font color="blue">The 29th USENIX Security Symposium (SEC'20)</font>*

*  **<font color="purple"> Interpretable Deep Learning under Fire </font>** [[pdf](https://arxiv.org/abs/1812.00891)]  <br>
*<font color="black">Xinyang Zhang, Ningfei Wang,  Hua Shen, Shouling Ji, Xiapu Luo, Ting
  Wang</font>* <br>
*<font color="blue">The 29th USENIX Security Symposium (SEC'20)</font>*
 
<hr>

### Code & Datasets

* i-Algebra Interactive Interpretation [[code](https://anonymous.4open.science/r/1684d602-17bd-432e-b59d-80bfd1dca5f7/)]

<hr>

<img src="/assets/images/nsf.jpg" alt="nsf" height="10%" width="10%" align="absmiddle"/> <img src="/assets/images/nvidia.jpg" alt="nvidia" height="12%" width="12%" align="absmiddle"/> We are grateful for the National Science Foundation (NSF) and Nvidia to support our research.

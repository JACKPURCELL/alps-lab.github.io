


Many of todays machine learning ML system are built by reusing an array of often pre trained primitive model each fulfilling distinct functionality e g  feature extraction
 The increasing use of primitive model significantly simplifies and expedites the development cycles of ML system  Yet because most of such model are contributed and maintained by untrusted sources their lack of standardization or regulation entails profound security implications about which little is known thus far


In this paper we demonstrate that malicious primitive model pose immense threats to the security of ML system  We present a broad class of {em model reuse} attack wherein maliciously crafted model trigger host ML system to malfunction on targeted input in a highly predictable manner  By empirically studying four ML system covering both individual and ensemble system used in skin cancer screening speech recognition face verification and autonomous steering we show that such attack are i effective  the host system misbehave on the targeted input as desired by the adversary with high probability ii evasive  the malicious model function indistinguishably from their benign counterparts on non targeted input iii elastic  the malicious model remain effective regardless of various system design choices and tuning strategies  and iv easy  the adversary does not need {em any} prior knowledge about the data used for system tuning or inference 

We provide analytical justification for the success of the model reuse attack which points to the unprecedented complexity of todays primitive model  The issue thus seems fundamental to many ML system  We further discuss potential countermeasures and their challenges which lead to several promising research directions

Privacy preserving releasing of complex data e g  image text audio represents a long standing challenge for the data mining research community  Due to rich semantics of the data and lack of {em a priori} knowledge about the analysis task excessive sanitization is often necessary to ensure privacy leading to significant loss of the data utility  Here we present system a general private releasing framework for semantic rich data  Instead of sanitizing and then releasing the data the data curator publishes a deep generative model which is trained using the original data in a differentially private manner with the generative model the analyst is able to produce an unlimited amount of synthetic data for arbitrary analysis tasks  In contrast of alternative solutions system highlights a set of key features: i it provides theoretical privacy guarantee via enforcing the differential privacy principle ii it retains desirable utility in the released model enabling a variety of otherwise impossible analyses and iii it achieves practical training stability by employing multi fold optimization strategies  Through extensive empirical evaluation on benchmark datasets and analyses we validate the efficacy of system

The recent abrupt advances in deep learning DL have led to breakthroughs in long standing artificial intelligence tasks  Yet like other machine learning techniques DL is inherently vulnerable to adversary input which are maliciously crafted samples to trigger deep neural networks dnn to misclassify leading to detrimental or even disastrous consequences for DL powered system e g  crashing of driverless vehicles  The fundamental challenges of defending against such attack stem from their adaptive and variable nature: adversary input are tailored to target dnn while crafting strategies vary greatly with concrete attack   Existing solutions attempt to improve DNN resilience against specific attack however the resulting static defenses once deployed can often be circumvented by adaptively engineered input or by new attack variants

Here we present EagleEye an attack agnostic adversary tampering analysis engine for DL powered system  Our design leverages the minimality principle'' underlying most attack: intuitively to maximize the attacks evasiveness an adversary input is crafted by applying the minimum possible distortion to a legitimate input  We show both empirically and analytically that this practice leads to the distinct distributional properties of adversary input in the input manifold space  By exploiting such properties in a principled manner EagleEye effectively discriminates adversary input and even uncovers their correct classification outputs  Through extensive empirical evaluation using a range of benchmark datasets and DNN architectures we validate EagleEyes efficacy: for instance on the SVHN dataset it detects adversary input with average recall of 94 6% and precision of 84 3% against varied attack and successfully infers the correct classification outputs for 74 3% of the adversary cases  Moreover we analyze the adversary possible countermeasures which however leads to a difficult dilemma for her: to evade EagleEyes detection excessive perturbations need to be applied thereby significantly reducing the attacks evasiveness regarding other anomaly detection mechanisms e g  human vision


Many of todays machine learning ML system are built by composing an array of primitive learning modules plms  The increasing use of plms significantly simplifies and expedites the system development cycles  However as most plms are contributed and maintained by third parties their lack of standardization and regulation entails profound security implications  In this paper for the first time we demonstrate that potentially harmful plms incur immense threats to the security of ML system  We present a broad class of logic bomb attack in which maliciously crafted plms trigger host ML system to malfunction in a predictable manner  By empirically studying a state of the art skin cancer screening system we validate the feasibility of such attack  For example it is shown highly probable to force the misdiagnosis of a target victim without prior knowledge about the building or the training of the host system  Further we present theoretical reasoning for the success of plm based attack which point to the characteristics of todays ML model: high dimension non linearity and non convexity  Thus the issue seems industry wide

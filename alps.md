---
title: ALPS Lab
layout: page
permalink: /alps/index.html
---


<img src="/assets/images/alps.png" alt="alps" height="75%" width="75%"/>

<hr>

### Overview

We are a team of faculty, post-doc scholars, and (under)grad students conducting cutting-edge research on the intersection of machine learning, computational privacy, and cyber-security.

Today, thanks to the availability of large-scale, information-rich data, data-driven algorithmic systems (e.g., personalized recommendation, autonomous driving, and precision medicine) are playing increasingly vital roles in our everyday lives. Our mission is to fully unleash the potential of such advances via:
* developing effective, scalable analytical tools to support ground-breaking discoveries and critical decision-making
* mitigating negative impacts on individuals and society as a whole, including privacy, security, and accountability issues.

We follow the methodology of "*from practice, to theory and back to practice*". We look to the real world for practical and important problems; then, we design solutions with provable properties and solid theoretical backing; finally, we build and deploy real systems based on these solutions. Our approach is multidisciplinary by nature, using "tools" from systems, algorithms, machine learning, cryptography, economics, and human-computer interactions.


<!-- My research focuses on modeling and mining of these dynamic, heterogeneous, and interdependent information sources. Towards this, my efforts involve developing new concepts and principles, designing intelligent algorithms, and building scalable systems. -->


<hr>

### People

I am fortunate to work and have worked with a group of wonderful students:

* [Ren Pang](https://ain-soph.github.io/)
* [Zhaohan Xi](https://ist.psu.edu/directory/zxx5113)
* [Changjiang Li](https://ist.psu.edu/directory/cbl5583)
* [Zheng Fu](https://ist.psu.edu/directory/fjd5166)
* [Suhang Cui](https://ist.psu.edu/directory/sxc6192) (co-supervision with Fenglong Ma)

#### Alumni


* [Xinyang Zhang](https://www.linkedin.com/in/xinyang-zhang-4580b8b7) -> Research Scientist at Baidu USA
* [Zheng Zhang](https://secantzhang.github.io) -> Ph.D. student at Northwestern
* [Ningfei Wang](https://www.linkedin.com/in/ningfei-wang-569a91156) -> Ph.D. student at UC Irvine
* Hua Shen -> Ph.D. student at Penn State
* [Yujie Ji](https://www.linkedin.com/in/yujie-ji-27484793) -> Software Engineer at Amazon
* [Chanh "Sam" Nguyen](https://www.linkedin.com/in/chanhnp) -> Technical Staff Member at Lawrence Livermore National Lab
* [Yifan Huang](https://www.linkedin.com/in/yifan-huang-303928156) -> Software Engineer at Bloomberg

<hr>

### Projects

Following is a list of our ongoing and past projects.


* **<font color="indianred">Extreme Scale Edge Learning for Healthcare</font>** <br> Diverse edge sensing devices are expected to reach 10-100 per person in the next decade, produce continuous, heterogeneous data at unprecedented volume, and enable numerous novel applications. We propose a new disciplinary concept of computational Screening and Surveillance (CSS) that utilizes edge learning to collect, analyze and interpret both physical and physiologic data of human subjects, to detect early indicators of diseases, and monitor health changes in both individuals and populations. This project will investigate and apply principles of extreme-scale edge learning to research challenges in four areas of algorithms & theory, security & privacy (S&P), systems, and architecture in CSS. The project website is [here]({{ site.baseurl }}/project/css).


* **<font color="indianred">Usable Interpretability</font>** <br> Deep neural network (DNN)-powered systems and services hold great promise to fundamentally transform the way people live, work and play.  Yet, to fully unleash this potential, it is critical to improve their interpretability to make them more trustworthy and easy-to-use. This project explores how to define and implement the interpretation of DNNs and how to exploit this interpretability as a bridge to understand and control the DNN behaviors. Specifically, we aim to develop a new interpretable deep learning framework that is reliable, interactive, and debuggable. The project website is [here]({{ site.baseurl }}/project/riddle).


* **<font color="indianred">Trustworthy Machine Learning from Untrusted Models</font>** <br>
Many of today's machine learning (ML)-based systems are not built from
scratch, but are "composed" from an array of pre-trained, third-party
models. Paralleling other forms of software reuse, reusing models can
both speed up and simplify the development of ML-based systems.
However, a lack of standardization, regulation, and verification of
third-party ML models raises security concerns. In particular, ML
models are subject to adversarial attacks in which third-party
attackers or model providers themselves might embed hidden behaviors
that are triggered by pre-specified inputs. This project aims at
understanding the security threats incurred by reusing third-party
models as building blocks of ML systems and developing tools to help
developers mitigate such threats throughout the lifecycle of ML
systems. The project website is [here]({{ site.baseurl }}/project/argus).



* **<font color="indianred">Attack-Agnostic Defenses against Adversarial Inputs</font>** <br>
Deep learning systems are inherently vulnerable to adversarial inputs, which are maliciously crafted samples to trigger deep neural networks (DNNs) to misbehave, leading to disastrous consequences in security-critical domains. The fundamental challenges of defending against such attacks stem from their adaptive and variable nature: adversarial inputs are tailored to target DNNs, while crafting strategies vary greatly with concrete attacks. In this project, we are building a universal, attack-agnostic defense framework that (i) works effectively against unseen attack variants, (ii) preserves the predictive power of DNNs, (iii) complements existing defense mechanisms, and (iv) provides comprehensive diagnosis about potential risks in the system outputs. The project website is [here]({{ site.baseurl }}/project/eagleeye).


* **<font color="indianred">Privacy-Aware Deep Learning of Contextual Knowledge</font>** <br>
Deep learning (DL) technology is envisioned to revolutionize contextual mobile services thanks to its capability of interpreting varied complex data available on mobile devices. However, with the great convenience and opportunities offered by DL-powered contextual services follows the immense threat to user privacy. In this project, we are building a Privacy-Aware Deep Learning of Contextual Knowledge engine, that facilitates the use of personal information from mobiles while maintaining explicit user control over how such information is used by third-party service providers. The project website is [here]({{ site.baseurl }}/project/padlock).

<!-- * **<font color="indianred">Crowd Wisdom in Open World</font>** <br>
Our decisions often rely on others' aggregated judgements, with the belief that the aggregations over a large population can successfully harness the "wisdom of crowds". However, in the open world, individuals are exposed to and "herded" by others' opinions before even forming their own, resulting in biased collective opinions. In this project, we conduct quantitative study on the dynamics underlying the crowd wisdom to answer the fundamental questions: How to characterize this herding effect? How to model its impact on systems that are constantly evolving? How to separate bias incurred by herding effects from genuine opinions? The project website is [here]({{ site.baseurl }}/project/wisdom). -->

<!-- * **<font color="indianred">Privacy-Aware Personalized Assistant for Healthcare Q&A </font>** <br>
Today, over one-third of Americans rely on online health forums (OHFs) to search and retrieve halthcare information. Compared with the increasing popularity of OHFs, the progress in their supporting platforms is lagging way behind. Built upon the traditional question-and-answer (Q&A) paradigm, todayâ€™s OHFs suffer two major issues: privacy vulnerability and Q&A inefficacy. In this project, we are building a personalized assistance tool that guides ordinary users to perform effective and privacy-preserving information seeking and providing on OHFs. The project website is [here]({{ site.baseurl }}/project/papaya). -->

<!-- * **<font color="indianred">Trustworthy Machine Learning from Untrusted Models</font>** <br> -->
